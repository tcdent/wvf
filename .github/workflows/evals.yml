name: Run Worldview Evaluations

on:
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      models:
        description: 'Models to evaluate (comma-separated, e.g., claude-sonnet,gpt-4o, or "all")'
        required: false
        default: 'all'
      difficulty:
        description: 'Difficulty level (baseline, moderate, extreme, or all)'
        required: false
        default: 'all'

  # Run on PR to evals directory
  pull_request:
    branches: [main, master]
    paths:
      - 'evals/**'
      - '.github/workflows/evals.yml'

jobs:
  evaluate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'evals/requirements.txt'

      - name: Install dependencies
        run: |
          pip install -r evals/requirements.txt

      - name: Download Worldview tools from latest build
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: build.yml
          name: worldview-tools
          path: ./bin
          search_artifacts: true
          if_no_artifact_found: fail

      - name: Make tools executable
        run: |
          chmod +x ./bin/*
          echo "${{ github.workspace }}/bin" >> $GITHUB_PATH

      - name: Parse model list
        id: models
        run: |
          MODELS="${{ github.event.inputs.models || 'all' }}"
          if [ "$MODELS" == "all" ]; then
            echo "use_all=true" >> $GITHUB_OUTPUT
            echo "list=" >> $GITHUB_OUTPUT
          else
            # Convert comma-separated to space-separated
            MODELS_SPACE="${MODELS//,/ }"
            echo "use_all=false" >> $GITHUB_OUTPUT
            echo "list=${MODELS_SPACE}" >> $GITHUB_OUTPUT
          fi

      - name: Run evaluations
        env:
          PYTHONUNBUFFERED: "1"
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          mkdir -p results

          # Build command
          CMD="python -m evals run --verbose --output results --worldview-cli ./bin/worldview"

          # Add models
          if [ "${{ steps.models.outputs.use_all }}" == "true" ]; then
            CMD="$CMD --all-models"
          else
            CMD="$CMD --models ${{ steps.models.outputs.list }}"
          fi

          # Add difficulty filter if not 'all'
          DIFFICULTY="${{ github.event.inputs.difficulty || 'all' }}"
          if [ "$DIFFICULTY" != "all" ]; then
            CMD="$CMD --difficulty $DIFFICULTY"
          fi

          echo "Running: $CMD"
          $CMD

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: eval-results-${{ github.run_number }}
          path: results/
          retention-days: 30

      - name: Post summary
        run: |
          echo "## Worldview Evaluation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat results/report.md >> $GITHUB_STEP_SUMMARY

      - name: Check for failures
        run: |
          # Extract success rate from JSON and fail if below threshold
          python -c "
          import json
          with open('results/results.json') as f:
              data = json.load(f)

          total_success = 0
          total_cases = 0

          for model, info in data['models'].items():
              total_success += info['summary']['success']
              total_cases += info['summary']['total']

          rate = total_success / total_cases if total_cases > 0 else 0
          print(f'Overall success rate: {rate:.1%}')

          # Baseline cases should always pass
          for model, info in data['models'].items():
              baseline_rate = info['summary']['baseline_rate']
              if baseline_rate < 0.8:
                  print(f'WARNING: {model} baseline rate {baseline_rate:.1%} is below 80%')
          "
